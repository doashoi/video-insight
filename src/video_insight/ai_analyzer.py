import os
import time
import base64
import json
import requests
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple

from openpyxl import Workbook
from openpyxl.drawing.image import Image as ExcelImage
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter

from .config import config
from .prompt_loader import prompt_loader

logger = logging.getLogger("AIAnalyzer")

class FeishuClient:
    """é£ä¹¦ Wiki/å¤šç»´è¡¨æ ¼ æ•°æ®è·å–å®¢æˆ·ç«¯ã€‚"""
    def __init__(self, app_id: str, app_secret: str):
        self.app_id = app_id
        self.app_secret = app_secret
        self.token = None
        self.headers = None

    def _ensure_token(self):
        """ç¡®ä¿å­˜åœ¨æœ‰æ•ˆçš„ tenant_access_tokenã€‚"""
        if not self.token:
            url = f"{config.FEISHU_DOMAIN}/open-apis/auth/v3/tenant_access_token/internal"
            payload = {"app_id": self.app_id, "app_secret": self.app_secret}
            try:
                res = requests.post(url, json=payload, timeout=10)
                res.raise_for_status()
                self.token = res.json().get("tenant_access_token")
                self.headers = {
                    "Authorization": f"Bearer {self.token}",
                    "Content-Type": "application/json; charset=utf-8"
                }
            except Exception as e:
                logger.error(f"è·å– Token å¤±è´¥: {e}")
                raise

    def get_app_token_from_wiki(self, wiki_token: str) -> Optional[str]:
        """è§£æ Wiki Token ä¸ºå¤šç»´è¡¨æ ¼ App Tokenã€‚"""
        self._ensure_token()
        url = f"{config.FEISHU_DOMAIN}/open-apis/wiki/v2/space_node/get"
        params = {"token": wiki_token}
        
        try:
            res = requests.get(url, headers=self.headers, params=params, timeout=10)
            res.raise_for_status()
            data = res.json().get("data", {})
            node = data.get("node", {})
            obj_type = node.get("obj_type")
            obj_token = node.get("obj_token")
            
            if obj_type != "bitable":
                logger.warning(f"Wiki èŠ‚ç‚¹ç±»å‹æ˜¯ '{obj_type}', é¢„æœŸä¸º 'bitable'ã€‚")
            
            return obj_token
        except Exception as e:
                logger.error(f"è§£æ Wiki èŠ‚ç‚¹å¤±è´¥: {e}")
                raise

    def get_all_records(self, app_token: str, table_id: str, view_id: str = None) -> List[Dict]:
        """è·å–å¤šç»´è¡¨æ ¼æ‰€æœ‰è®°å½•ã€‚"""
        self._ensure_token()
        all_records = []
        page_token = ""
        has_more = True
        
        logger.info("æ­£åœ¨ä»é£ä¹¦å¤šç»´è¡¨æ ¼è·å–æ•°æ®...")
        while has_more:
            url = f"{config.FEISHU_DOMAIN}/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
            params = {"page_size": 100, "page_token": page_token}
            if view_id:
                params["view_id"] = view_id
                
            try:
                res = requests.get(url, headers=self.headers, params=params, timeout=20)
                res.raise_for_status()
                data = res.json().get("data", {})
                
                items = data.get("items", [])
                all_records.extend(items)
                
                has_more = data.get("has_more", False)
                page_token = data.get("page_token", "")
            except Exception as e:
                logger.error(f"è·å–è®°å½•å¤±è´¥: {e}")
                break
        
        logger.info(f"æˆåŠŸè·å– {len(all_records)} æ¡è®°å½•ã€‚")
        return all_records

class AdsAnalyzer:
    def __init__(self, output_dir: Path = None, assets_dir: Path = None):
        self.output_dir = output_dir or config.RESULT_DIR
        self.assets_dir = assets_dir or config.OUTPUT_DIR
        self.api_key = config.DASHSCOPE_API_KEY
        self.feishu_client = FeishuClient(config.FEISHU_APP_ID, config.FEISHU_APP_SECRET)
        
        if not self.api_key:
            logger.warning("ç¯å¢ƒå˜é‡ä¸­æœªæ‰¾åˆ° DASHSCOPE_API_KEYã€‚")

        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _encode_image(self, image_path: str) -> str:
        """å°†å›¾åƒç¼–ç ä¸º base64ã€‚"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def _call_dashscope(self, system_prompt: str, user_content: List[Dict], model: str = "qwen-vl-plus-2025-08-15") -> Optional[str]:
        """é€šç”¨ DashScope API è°ƒç”¨æ–¹æ³•ã€‚"""
        url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation"
        if "vl" not in model.lower():
             # å¦‚æœæ˜¯éè§†è§‰æ¨¡å‹ï¼Œä½¿ç”¨ä¸åŒçš„ URL (è™½ç„¶ Qwen-VL ä¹Ÿèƒ½å¤„ç†çº¯æ–‡æœ¬ï¼Œä½†ä¸ºäº†æ‰©å±•æ€§ä¿ç•™æ­¤åˆ¤æ–­)
             url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "input": {
                "messages": [
                    {"role": "system", "content": [{"text": system_prompt}]},
                    {"role": "user", "content": user_content}
                ]
            },
            "parameters": {
                "result_format": "message"
            }
        }

        for attempt in range(3):
            try:
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                
                if "output" in result and "choices" in result["output"]:
                    content = result["output"]["choices"][0]["message"]["content"][0]["text"]
                    return content
                else:
                    logger.error(f"æ„å¤–å“åº”: {result}")
            
            except Exception as e:
                logger.error(f"ç¬¬ {attempt+1}/3 æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < 2:
                    time.sleep(2)
        return None

    def _get_visual_description(self, image_path: str, text_content: str) -> Optional[str]:
        """ç¬¬ä¸€é˜¶æ®µï¼šè§†è§‰å†…å®¹è¯†åˆ«ï¼ˆç»“åˆç”»é¢ä¸æ–‡æ¡ˆï¼‰ã€‚"""
        system_prompt = prompt_loader.load("video_analyzer/visual_description.md")
        user_content = [
            {"image": f"data:image/jpeg;base64,{self._encode_image(image_path)}"},
            {"text": f"ã€è¯­éŸ³æ–‡æ¡ˆã€‘ï¼š\n{text_content}\n\nè¯·ç»“åˆæ–‡æ¡ˆï¼Œå®¢è§‚æè¿°è¯¥è§†é¢‘å®«æ ¼å›¾å‘ˆç°çš„å†…å®¹ã€‚"}
        ]
        return self._call_dashscope(system_prompt, user_content)

    def _synthesize_analysis(self, visual_desc: str, text_content: str, row_data: Dict, schema: List[Dict] = None, user_logic: str = "") -> Optional[Dict]:
        """ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æ•´åˆåˆ†æã€‚æ ¹æ® Schema åŠ¨æ€ç”Ÿæˆåˆ†æé€»è¾‘ã€‚"""
        system_prompt = prompt_loader.load("video_analyzer/data_synthesis.md")
        
        # å¦‚æœæä¾›äº† Schemaï¼ŒåŠ¨æ€å¢å¼ºæç¤ºè¯
        schema_instruction = ""
        if schema:
            schema_instruction = "\n\n# Output Field Constraints (Strictly follow this Schema)\n"
            schema_instruction += "ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹å­—æ®µå®šä¹‰è¿›è¡Œåˆ†æï¼Œä¸è¦è¾“å‡ºä»»ä½•ä¸åœ¨åˆ—è¡¨ä¸­çš„å­—æ®µã€‚\n"
            for field in schema:
                name = field["field_name"]
                f_type = field["type"]
                # æ’é™¤ä¸€äº›ç³»ç»Ÿå­—æ®µæˆ–åªè¯»å­—æ®µ
                if name in ["ç¼©ç•¥å›¾", "è§†é¢‘é“¾æ¥", "ç´ æåç§°"]:
                    continue
                
                desc = f"- **{name}** (ç±»å‹ä»£ç : {f_type})"
                if "options" in field:
                    desc += f" | å¿…é¡»ä»ä»¥ä¸‹é¢„è®¾é€‰é¡¹ä¸­é€‰æ‹©: {field['options']}"
                schema_instruction += desc + "\n"
            
            schema_instruction += "\nè¯·åŠ¡å¿…è¿”å›ä¸€ä¸ªåŒ…å«ä¸Šè¿°å­—æ®µçš„ JSON å¯¹è±¡ã€‚å¦‚æœæŸä¸ªå­—æ®µæ— æ³•ä»å†…å®¹ä¸­å¾—å‡ºï¼Œè¯·ä¿æŒä¸ºç©ºå­—ç¬¦ä¸²æˆ–é»˜è®¤å€¼ã€‚"

        # å‡†å¤‡è¾“å…¥æ•°æ® (åŒ…å«æ‰€æœ‰è¡Œæ•°æ®)
        # æ’é™¤å·²çŸ¥çš„åª’ä½“å­—æ®µï¼Œå°†å…¶ä»–æ‰€æœ‰å­—æ®µä½œä¸ºâ€œå‚è€ƒæ•°æ®â€å–‚ç»™ AI
        reference_data = {k: v for k, v in row_data.items() if k not in ["ç´ æåç§°", "è§†é¢‘é“¾æ¥", "ç¼©ç•¥å›¾"]}
        data_str = json.dumps(reference_data, ensure_ascii=False, indent=2)
        
        user_input = f"""
ã€è§†è§‰æè¿°ã€‘ï¼š
{visual_desc}

ã€è¯­éŸ³æ–‡æ¡ˆã€‘ï¼š
{text_content}

ã€å‚è€ƒæ•°æ®ï¼ˆåŒ…å«åŸå§‹è¡¨æ ¼ä¸­çš„æ‰€æœ‰å­—æ®µä¿¡æ¯ï¼‰ã€‘ï¼š
{data_str}

ã€ç”¨æˆ·ç¡®è®¤çš„åˆ†æé€»è¾‘/æŒ‡ä»¤ã€‘ï¼š
{user_logic or "è¯·æŒ‰é»˜è®¤é€»è¾‘è¿›è¡Œæ·±åº¦åˆ†æã€‚"}
"""
        full_system_prompt = system_prompt + schema_instruction
        user_content = [{"text": user_input}]
        
        # ä½¿ç”¨è§†è§‰æ¨¡å‹å¤„ç†çº¯æ–‡æœ¬ (Qwen-VL ä¹Ÿèƒ½å¤„ç†)
        response_text = self._call_dashscope(full_system_prompt, user_content, model="qwen-vl-plus-2025-08-15")
        
        if response_text:
            try:
                content = response_text.replace("```json", "").replace("```", "").strip()
                return json.loads(content)
            except Exception as e:
                logger.error(f"è§£æ JSON å¤±è´¥: {e}\nåŸå†…å®¹: {response_text}")
        return None

    def analyze_template(self, fields: List[Dict]) -> Optional[List[Dict]]:
        """è§£æç”¨æˆ·æä¾›çš„é£ä¹¦æ¨¡æ¿æ„å›¾ã€‚ç”Ÿæˆç†è§£æ¸…å•ã€‚"""
        system_prompt = prompt_loader.load("interaction/intent_clarification.md")
        
        # ç®€åŒ–å­—æ®µä¿¡æ¯ä¼ ç»™ AI
        simplified_fields = []
        for f in fields:
            f_info = {
                "field_name": f.get("field_name"),
                "field_type": f.get("type"),
            }
            if "options" in f:
                f_info["options"] = f["options"]
            simplified_fields.append(f_info)

        fields_str = json.dumps(simplified_fields, ensure_ascii=False, indent=2)
        user_input = f"ä»¥ä¸‹æ˜¯æˆ‘çš„é£ä¹¦å¤šç»´è¡¨æ ¼æ¨¡æ¿å­—æ®µåˆ—è¡¨ï¼Œè¯·æŒ‰è§„èŒƒç”Ÿæˆç†è§£ç¡®è®¤æ¸…å•ï¼š\n{fields_str}"
        
        user_content = [{"text": user_input}]
        # ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹æ¥åšé€»è¾‘åˆ†æ
        response_text = self._call_dashscope(system_prompt, user_content, model="qwen-max")
        
        if response_text:
            try:
                content = response_text.replace("```json", "").replace("```", "").strip()
                return json.loads(content)
            except Exception as e:
                logger.error(f"æ¨¡æ¿æ„å›¾è§£æå¤±è´¥: {e}\nå†…å®¹: {response_text}")
        return None

    def _find_assets(self, material_name: str) -> Tuple[Optional[str], Optional[str]]:
        """åœ¨èµ„æºç›®å½•ä¸­æŸ¥æ‰¾æ‹¼å›¾å’Œ ASR æ–‡æ¡ˆã€‚"""
        # èµ„æºç›®å½•é€šå¸¸æ˜¯å¤„ç†åçš„ç»“æœç›®å½•: assets_dir / material_name / (material_name_sheet.jpg & material_name_asr.txt)
        material_dir = self.assets_dir / material_name
        
        sheet_path = material_dir / f"{material_name}_sheet.jpg"
        text_path = material_dir / f"{material_name}_asr.txt"
        
        if sheet_path.exists() and text_path.exists():
            return str(sheet_path), str(text_path)
            
        return None, None

    def _fetch_feishu_data(self, app_token: str = None, table_id: str = None) -> List[Dict]:
        """è·å–å¹¶æ ‡å‡†åŒ–é£ä¹¦æ•°æ®ã€‚åŠ¨æ€æå–æ‰€æœ‰å¯ç”¨å­—æ®µã€‚"""
        target_app_token = app_token
        target_table_id = table_id

        if not target_app_token:
            logger.info("æ­£åœ¨ä» Wiki Token è§£æ App Token...")
            target_app_token = self.feishu_client.get_app_token_from_wiki(config.WIKI_TOKEN)
        
        if not target_app_token:
            logger.error("è·å– App Token å¤±è´¥")
            return []
            
        logger.info(f"App Token: {target_app_token}")
        if not target_table_id:
             target_table_id = config.SOURCE_TABLE_ID
             
        records = self.feishu_client.get_all_records(target_app_token, target_table_id)
        
        normalized_data = []
        for r in records:
            fields = r.get("fields", {})
            item = {}
            
            # åŠ¨æ€å¤„ç†æ‰€æœ‰å­—æ®µ
            for key, val in fields.items():
                # å¤„ç†å¸¸è§çš„é£ä¹¦å¤æ‚å­—æ®µç±»å‹
                if isinstance(val, list) and len(val) > 0:
                    item_0 = val[0]
                    if isinstance(item_0, dict):
                        # å¤„ç†é“¾æ¥ã€æ–‡æœ¬ã€äººå‘˜ç­‰
                        item[key] = item_0.get("url") or item_0.get("link") or item_0.get("text") or item_0.get("name") or str(val)
                    else:
                        item[key] = val
                elif isinstance(val, dict):
                    item[key] = val.get("url") or val.get("link") or val.get("text") or val.get("name") or str(val)
                else:
                    item[key] = val
            
            # ç¡®ä¿å…³é”®å­—æ®µå­˜åœ¨ï¼ˆå³ä½¿ä¸ºç©ºï¼‰
            if "ç´ æåç§°" not in item:
                # å°è¯•é€šè¿‡åˆ«åæˆ–æœç´¢å«æœ‰â€œè§†é¢‘â€æˆ–â€œåç§°â€çš„å­—æ®µä½œä¸ºç´ æå
                for k in item.keys():
                    if "åç§°" in k or "è§†é¢‘" in k or "ç´ æ" in k:
                        item["ç´ æåç§°"] = item[k]
                        break

            normalized_data.append(item)
            
        return normalized_data

    def process(self, source_app_token: str = None, source_table_id: str = None, progress_callback=None, schema: List[Dict] = None, user_logic: str = "") -> List[Dict]:
        """æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šè¯»å–æºè¡¨ï¼Œåˆ†æè§†é¢‘ï¼Œè¿”å›ç»“æœåˆ—è¡¨ã€‚"""
        # ... ä¿æŒä¹‹å‰çš„é€»è¾‘ ...
        target_app_token = source_app_token
        if not target_app_token:
            logger.info("æ­£åœ¨ä» Wiki Token è§£æ App Token...")
            target_app_token = self.feishu_client.get_app_token_from_wiki(config.SOURCE_WIKI_TOKEN)
            
        if not target_app_token:
            logger.error("è·å– App Token å¤±è´¥")
            return []
            
        logger.info(f"App Token: {target_app_token}")
        target_table_id = source_table_id or config.SOURCE_TABLE_ID

        # è·å–æ•°æ® (æ­¤æ—¶å·²æ˜¯åŠ¨æ€æå–)
        data = self._fetch_feishu_data(target_app_token, target_table_id)
        
        results = []
        total_rows = len(data)
        logger.info(f"å‘ç° {total_rows} è¡Œå¾…å¤„ç†æ•°æ®ã€‚")
        
        success_count = 0
        skip_count = 0
        
        report_step = max(1, total_rows // 5) if total_rows > 10 else 1

        for index, row in enumerate(data):
            material_name = str(row.get('ç´ æåç§°', ''))
            if material_name.lower().endswith('.mp4'):
                material_name = material_name[:-4]
            material_name = material_name.strip()

            if not material_name:
                skip_count += 1
                continue
            
            if progress_callback:
                progress_callback(f"ğŸ¤– [3/4] æ­£åœ¨åˆ†æ ({index+1}/{total_rows}): {material_name}")
                
            logger.info(f"æ­£åœ¨å¤„ç†: {material_name}")
            
            # æŸ¥æ‰¾ç´ æ
            sheet_path, text_path = self._find_assets(material_name)
            if not sheet_path or not text_path:
                logger.warning(f"{material_name}: æœªæ‰¾åˆ°æœ¬åœ°ç´ æ (è·³è¿‡åˆ†æ)")
                skip_count += 1
                if progress_callback:
                    progress_callback(f"âš ï¸ {material_name}: æœªæ‰¾åˆ°æœ¬åœ°ç´ æ (è·³è¿‡åˆ†æ)")
                continue

            # è¯»å–æ–‡æ¡ˆ
            try:
                with open(text_path, "r", encoding="utf-8") as f:
                    text_content = f.read()
            except Exception as e:
                logger.error(f"è¯»å–æ–‡æ¡ˆå¤±è´¥: {e}")
                skip_count += 1
                continue

            # è°ƒç”¨ AI (ä¸¤é˜¶æ®µåˆ†æï¼Œä¼ å…¥ Schema å’Œç”¨æˆ·ç¡®è®¤åçš„é€»è¾‘)
            visual_desc = self._get_visual_description(sheet_path, text_content)
            analysis_json = None
            if visual_desc:
                analysis_json = self._synthesize_analysis(visual_desc, text_content, row, schema=schema, user_logic=user_logic)

            if analysis_json:
                # åˆå¹¶ç»“æœ (ä¼˜å…ˆä½¿ç”¨åˆ†æç»“æœè¦†ç›–åŸå§‹æ•°æ®)
                res_item = {**row, **analysis_json}
                
                # æ˜¾å¼æ·»åŠ ç¼©ç•¥å›¾è·¯å¾„
                if sheet_path and os.path.exists(sheet_path):
                    res_item["ç¼©ç•¥å›¾"] = sheet_path
                
                results.append(res_item)
                success_count += 1
                
                if progress_callback:
                    if total_rows <= 10:
                        pass 
                    elif (success_count % report_step == 0) or (index + 1 == total_rows):
                        progress_callback(f"ğŸ“Š AI åˆ†æè¿›åº¦: {index+1}/{total_rows} (å·²å®Œæˆ {success_count} æ¡)")
            else:
                skip_count += 1
                if progress_callback:
                    progress_callback(f"âŒ {material_name}: AI åˆ†æå¤±è´¥")

        if progress_callback:
            progress_callback(f"âœ… AI åˆ†æå…¨éƒ¨å®Œæˆï¼Œç”Ÿæˆ {len(results)} æ¡ç»“æœã€‚")
            
        return results

    # ä¿ç•™æ—§æ–¹æ³•ä»¥å…¼å®¹ CLI (å¦‚æœéœ€è¦)ï¼Œä½†åœ¨æ–°ç®¡çº¿ä¸­æœªä½¿ç”¨
    def _save_excel(self, results: List[Dict]):
        pass

if __name__ == "__main__":
    logger.info("ğŸš€ å¼€å§‹å¹¿å‘Šåˆ†æ...")
    analyzer = AdsAnalyzer()
    results = analyzer.process()
    # print(json.dumps(results, ensure_ascii=False, indent=2))
