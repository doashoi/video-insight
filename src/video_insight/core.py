import threading
import traceback
import os
import shutil
import logging
import re
import time
import sys
from datetime import datetime
from typing import Optional, Tuple
from pathlib import Path

import lark_oapi
from lark_oapi.api.wiki.v2.model import GetNodeSpaceRequest

from video_insight.config import config
from video_insight.downloader import run_downloader
from video_insight.video_processor import process_video_folder
from video_insight.ai_analyzer import AdsAnalyzer
from video_insight.feishu_syncer import FeishuSyncer

logger = logging.getLogger("Core")

# å…¨å±€å†…å­˜é”ï¼Œç”¨äºå•è¿›ç¨‹å†…çš„çº¿ç¨‹åŒæ­¥
TASK_LOCK = threading.Lock()

def resolve_wiki_token(wiki_token: str) -> Tuple[Optional[str], Optional[str]]:
    """
    é€šè¿‡ Wiki Token è§£æå‡ºå¯¹åº”çš„ Bitable App Tokenã€‚
    """
    logger.info(f"Resolving wiki token: {wiki_token}")
    client = lark_oapi.Client.builder().app_id(config.FEISHU_APP_ID).app_secret(config.FEISHU_APP_SECRET).build()
    try:
        req = GetNodeSpaceRequest.builder() \
            .token(wiki_token) \
            .build()
        resp = client.wiki.v2.space.get_node(req)
        
        if not resp.success():
            logger.error(f"Failed to resolve wiki token: {resp.msg}")
            return None, None
            
        node = resp.data.node
        if node.obj_type == "bitable":
            logger.info(f"Resolved wiki token to bitable: {node.obj_token}")
            return node.obj_token, None # table_id æ— æ³•ä» wiki token ç›´æ¥è·å–ï¼Œé€šå¸¸é»˜è®¤ä¸ºç¬¬ä¸€ä¸ªè¡¨
        else:
            logger.warning(f"Wiki node is not a bitable: {node.obj_type}")
            return None, None
    except Exception as e:
        logger.error(f"Error resolving wiki token: {e}")
        return None, None

def parse_feishu_url(url: str) -> Tuple[Optional[str], Optional[str], str]:
    """
    è§£æé£ä¹¦é“¾æ¥ï¼Œæ”¯æŒå¤šç»´è¡¨æ ¼é“¾æ¥å’ŒçŸ¥è¯†åº“é“¾æ¥ã€‚
    è¿”å› (app_token, table_id, domain)
    """
    try:
        url = url.strip() # Remove whitespace
        logger.info(f"Parsing URL: {url}")
        
        # æå–åŸŸå
        domain_match = re.search(r"https?://([^/]+)", url)
        domain = domain_match.group(0) if domain_match else config.FEISHU_DOMAIN
        
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ Wiki é“¾æ¥
        wiki_match = re.search(r"\/wiki\/([a-zA-Z0-9]+)", url)
        if wiki_match:
            wiki_token = wiki_match.group(1)
            logger.info(f"Detected Wiki link, token: {wiki_token}")
            app_token, table_id = resolve_wiki_token(wiki_token)
            return app_token, table_id, domain

        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯æ™®é€šçš„ Base é“¾æ¥
        if "/base/" in url:
            part1 = url.split("/base/")[1]
            app_token = part1.split("?")[0].split("/")[0]
            
            table_id = None
            if "table=" in url:
                table_id = url.split("table=")[1].split("&")[0]
                
            return app_token, table_id, domain
            
        return None, None, domain
    except Exception as e:
        logger.error(f"Error parsing Feishu URL: {e}")
        return None, None, config.FEISHU_DOMAIN

def cleanup_temp_files(folders: list = None):
    """æ¸…ç†ä¸´æ—¶ä¸‹è½½å’Œå¤„ç†ç›®å½•ã€‚"""
    if folders is None:
        folders = [config.OUTPUT_DIR, config.RESULT_DIR]
    
    for folder in folders:
        if folder.exists():
            # å‡å°‘æ—¥å¿—è¾“å‡ºï¼Œé™¤éæ˜¯ DEBUG æ¨¡å¼
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"Cleaning up folder: {folder}")
            try:
                # åˆ é™¤æ–‡ä»¶å¤¹å†…æ‰€æœ‰å†…å®¹ä½†ä¿ç•™æ–‡ä»¶å¤¹æœ¬èº«
                for item in folder.iterdir():
                    if item.is_file():
                        item.unlink()
                    elif item.is_dir():
                        shutil.rmtree(item)
                # å°è¯•åˆ é™¤æ–‡ä»¶å¤¹æœ¬èº«ï¼ˆå¦‚æœæ˜¯åŠ¨æ€ç”Ÿæˆçš„ç¼“å­˜ç›®å½•ï¼‰
                try:
                    folder.rmdir()
                except OSError:
                    pass 
            except Exception as e:
                logger.error(f"Failed to cleanup {folder}: {e}")

def run_pipeline_task(user_id: str, source_url: str, progress_callback=None, template_url: str = None):
    """
    æ‰§è¡Œå®Œæ•´çš„å¤„ç†ç®¡çº¿ã€‚
    """
    def report_progress(msg):
        logger.info(f"[Progress] {msg}")
        sys.stdout.flush() # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—
        if progress_callback:
            try:
                progress_callback(msg)
            except Exception as e:
                logger.warning(f"Failed to send progress update: {e}")

    report_progress("ğŸš€ å¼€å§‹æ‰§è¡Œè§†é¢‘æ´å¯Ÿåˆ†æä»»åŠ¡...")
    
    # åŠ¨æ€ç”Ÿæˆç¼“å­˜ç›®å½•
    cache_root_dir = None
    video_download_dir = None
    result_dir = None

    try:
        # --- æ­¥éª¤ 0: è§£ææº ---
        syncer = FeishuSyncer()
        report_progress("ğŸ” æ­£åœ¨è§£ææºè¡¨æ ¼é“¾æ¥...")
        source_app_token, source_table_id, domain = parse_feishu_url(source_url)
        
        if not source_app_token:
             return False, None, "æ— æ³•è§£ææºè¡¨æ ¼é“¾æ¥ï¼Œè¯·ç¡®ä¿é“¾æ¥æ­£ç¡®ä¸”æœºå™¨äººæœ‰æƒé™è®¿é—®ã€‚"
        
        # è·å–åŸè¡¨åç§°
        original_name = syncer.get_app_name(source_app_token) or "æœªå‘½åè¡¨æ ¼"
        # ç§»é™¤å¯èƒ½ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
        safe_name = re.sub(r'[\\/*?:"<>|]', "", original_name)
        report_progress(f"ğŸ“‹ å·²å®šä½æºè¡¨æ ¼: {original_name}")

        # è®¾ç½®ä¸´æ—¶ç›®å½•
        # ç»Ÿä¸€ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„å­ç›®å½•ï¼Œé¿å…æ±¡æŸ“ç”¨æˆ·æ¡Œé¢
        if config.IS_FC:
            cache_root_dir = Path("/tmp") / f"task_{user_id}_{int(time.time())}"
        else:
            # æœ¬åœ°ç¯å¢ƒä¸‹ä½¿ç”¨ .cache ç›®å½•
            cache_root_dir = config.ROOT_DIR / ".cache" / f"task_{user_id}_{int(time.time())}"
            
        video_download_dir = cache_root_dir / "downloads"
        result_dir = cache_root_dir / "results"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        video_download_dir.mkdir(parents=True, exist_ok=True)
        result_dir.mkdir(parents=True, exist_ok=True)
        
        # report_progress(f"ğŸ“‚ ä¸´æ—¶å·¥ä½œç›®å½•: {cache_root_dir}")

        # --- æ­¥éª¤ 1: å‡†å¤‡ç›®æ ‡ç©ºé—´ ---
        report_progress("ğŸ“‚ æ­£åœ¨å‡†å¤‡â€œè‡ªåŠ¨åˆ†æâ€ç©ºé—´...")
        folder_token = syncer.get_or_create_folder("è‡ªåŠ¨åˆ†æ", user_id)
        if not folder_token:
            return False, None, "æ— æ³•åœ¨æ‚¨çš„ç©ºé—´åˆ›å»ºâ€œè‡ªåŠ¨åˆ†æâ€æ–‡ä»¶å¤¹ã€‚"

        # --- æ­¥éª¤ 2: åˆ›å»ºç»“æœè¡¨æ ¼ ---
        full_app_name = f"{original_name}_è‡ªåŠ¨åˆ†æ"
        report_progress(f"ğŸ†• æ­£åœ¨åˆ›å»ºç»“æœè¡¨: {full_app_name} ...")
        
        # å†³å®šå¤åˆ¶æº
        copy_source_app_token = source_app_token
        if template_url and template_url.strip():
            report_progress("ğŸ¨ æ­£åœ¨è§£ææ¨¡æ¿è¡¨æ ¼é“¾æ¥...")
            template_app_token, _, _ = parse_feishu_url(template_url)
            if template_app_token:
                copy_source_app_token = template_app_token
                report_progress("âœ¨ å·²åˆ‡æ¢è‡³ç”¨æˆ·è‡ªå®šä¹‰æ¨¡æ¿ã€‚")
            else:
                report_progress("âš ï¸ æ¨¡æ¿é“¾æ¥è§£æå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸè¡¨ç»“æ„ä½œä¸ºå…œåº•ã€‚")

        # ä½¿ç”¨ copy_bitable æ›¿ä»£ create_bitableï¼Œä»¥ä¿ç•™åŸè¡¨ç»“æ„
        app_token = syncer.copy_bitable(copy_source_app_token, full_app_name, folder_token, user_id)
        if not app_token:
            error_msg = getattr(syncer, 'last_error', None) or "å¤åˆ¶å¤šç»´è¡¨æ ¼å¤±è´¥"
            return False, None, error_msg
        
        # --- æ­¥éª¤ 3: åˆå§‹åŒ–æƒé™å’Œè·å– Schema ---
        syncer.add_member_permission(app_token, user_id)
        
        table_id = syncer.get_default_table_id(app_token)
        if not table_id:
            return False, None, "æ— æ³•è·å–æ–°è¡¨çš„é»˜è®¤æ•°æ®è¡¨ ID"
            
        # è·å–ç›®æ ‡è¡¨çš„ç»“æ„å®šä¹‰
        report_progress("ğŸ“‹ æ­£åœ¨è·å–ç›®æ ‡è¡¨ç»“æ„å®šä¹‰...")
        schema = syncer.get_table_schema(app_token, table_id)
        if not schema:
             report_progress("âš ï¸ æ— æ³•è·å–è¡¨ç»“æ„ï¼Œå°†ä½¿ç”¨é»˜è®¤åˆ†æé€»è¾‘ã€‚")
        else:
             report_progress(f"âœ… å·²æˆåŠŸè§£æ {len(schema)} ä¸ªå­—æ®µå®šä¹‰ã€‚")

        # --- æ­¥éª¤ 4: æ„å›¾ç¡®è®¤ä¸ä¸»åŠ¨è¿½é—® (æ–°å¢ç¯èŠ‚) ---
        report_progress("ğŸ¤” æ­£åœ¨ç”Ÿæˆåˆ†ææ„å›¾ç¡®è®¤æ¸…å•...")
        analyzer = AdsAnalyzer(output_dir=result_dir, assets_dir=result_dir)
        confirmation_list = analyzer.analyze_template(schema)
        
        if confirmation_list:
            # è¿™é‡Œçš„é€»è¾‘åœ¨å®é™…ç”Ÿäº§ä¸­åº”è¯¥ï¼š
            # 1. å‘é€æ¶ˆæ¯å¡ç‰‡ç»™ç”¨æˆ·
            # 2. ç­‰å¾…ç”¨æˆ·ç¡®è®¤æˆ–ä¿®æ”¹æŒ‡ä»¤
            # 3. å¦‚æœç”¨æˆ·ä¿®æ”¹ï¼Œåˆ™æ›´æ–° user_logic é‡æ–°åˆ†ææˆ–ç›´æ¥åº”ç”¨
            # ç›®å‰ä½œä¸º MVP é˜¶æ®µï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹æˆ–å°†æ¸…å•è®°å½•åˆ°æ—¥å¿—ä¸­
            report_progress("ğŸ“ AI å¯¹å½“å‰æ¨¡æ¿çš„ç†è§£å¦‚ä¸‹ï¼š")
            for item in confirmation_list:
                status_icon = "âœ…" if item['status'] == 'resolved' else "â“"
                report_progress(f"{status_icon} ã€{item['field_name']}ã€‘: {item['logic_description']}")
                if item['status'] != 'resolved':
                    report_progress(f"   ğŸ‘‰ è¿½é—®: {item['clarification_question']}")
            
            # TODO: è¿™é‡Œéœ€è¦ä¸€ä¸ªçœŸæ­£çš„äº¤äº’å¾ªç¯
            # user_logic = wait_for_user_confirmation(confirmation_list)
            user_logic = "" # æš‚æ—¶ç•™ç©ºï¼Œè¡¨ç¤ºä½¿ç”¨ AI é»˜è®¤ç”Ÿæˆçš„é€»è¾‘
        else:
            user_logic = ""

        # å‘ŠçŸ¥ç”¨æˆ·æ–°è¡¨é“¾æ¥
        # ç¡®ä¿åŸŸåä¸åŒ…å«å¤šä½™å­—ç¬¦
        clean_domain = domain.rstrip("/")
        table_url = f"{clean_domain}/base/{app_token}?table={table_id}"
        report_progress(f"âœ… ç»“æœè¡¨å·²å‡†å¤‡å°±ç»ªï¼\nğŸ”— é“¾æ¥: {table_url}\n\nç°åœ¨å¼€å§‹å¤„ç†è§†é¢‘ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·ç¨åæŸ¥çœ‹ç»“æœè¡¨ã€‚")

        # --- æ­¥éª¤ 5: è¿è¡Œä¸‹è½½ä¸åˆ†æç®¡çº¿ ---
        # 5.1 ä¸‹è½½è§†é¢‘
        report_progress("â¬‡ï¸ [1/4] æ­£åœ¨ä¸‹è½½è§†é¢‘...")
        run_downloader(source_app_token, source_table_id, report_progress, output_dir=video_download_dir)
        
        # 5.2 å¤„ç†è§†é¢‘ (VAD/ASR)
        report_progress("ğŸµ [2/4] æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ« (ASR)...")
        process_video_folder(video_download_dir, result_dir, report_progress)
        
        # 5.3 AI åˆ†æ (ä¼ å…¥ user_logic)
        report_progress("ğŸ¤– [3/4] æ­£åœ¨ä½¿ç”¨ AI åˆ†æè§†é¢‘å†…å®¹...")
        analysis_results = analyzer.process(source_app_token, source_table_id, report_progress, schema=schema, user_logic=user_logic) 
        
        # 5.4 åŒæ­¥åˆ°æ–°è¡¨æ ¼
        report_progress(f"ğŸ”„ [4/4] æ­£åœ¨åŒæ­¥ {len(analysis_results)} æ¡åˆ†æç»“æœåˆ°é£ä¹¦...")
        syncer.sync_data(analysis_results, app_token, table_id)
        
        # ä»»åŠ¡å®Œæˆåæ¸…ç†
        if cache_root_dir and cache_root_dir.exists():
            # report_progress(f"ğŸ§¹ æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {cache_root_dir}")
            shutil.rmtree(cache_root_dir, ignore_errors=True)
        
        return True, app_token, full_app_name
        
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        logger.error(traceback.format_exc())
        # å‡ºé”™æ—¶ä¹Ÿå°è¯•æ¸…ç†
        if cache_root_dir and cache_root_dir.exists():
             shutil.rmtree(cache_root_dir, ignore_errors=True)
        return False, None, str(e)
