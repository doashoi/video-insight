import os
import sys
import subprocess
import gc
import re
import math
import shutil
import traceback
import json
import requests
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union, Any

import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont

from .config import config

class VideoAnalyzer:
    def __init__(self):
        """ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„åˆå§‹åŒ– VideoAnalyzerã€‚"""
        self.ffmpeg_exe = config.FFMPEG_PATH
        self.api_key = config.DASHSCOPE_API_KEY
        
        # æ³¨å†Œ FFmpeg è·¯å¾„
        ffmpeg_dir = os.path.dirname(str(self.ffmpeg_exe))
        if ffmpeg_dir and ffmpeg_dir not in os.environ["PATH"]:
            os.environ["PATH"] = ffmpeg_dir + os.pathsep + os.environ["PATH"]
            print(f"[Init] FFmpeg è·¯å¾„å·²æ³¨å†Œ: {ffmpeg_dir}")

    def release_model(self):
        """é‡Šæ”¾èµ„æºã€‚"""
        gc.collect()

    def extract_audio_track(self, video_path: str, audio_path: str) -> bool:
        """ä»è§†é¢‘æå–éŸ³é¢‘ (16k, mono, pcm_s16le)ã€‚"""
        # æ³¨æ„ï¼šDashScope ASR æ”¯æŒå¤šç§æ ¼å¼ï¼Œä½† 16k mono wav æ˜¯æœ€é€šç”¨çš„
        cmd = [
            str(self.ffmpeg_exe), "-y", "-i", video_path,
            "-vn", "-c:a", "pcm_s16le", "-ar", "16000", "-ac", "1",
            "-f", "wav", audio_path, "-loglevel", "error"
        ]
        try:
            print(f"[Audio] æ­£åœ¨æå–éŸ³é¢‘: {Path(video_path).name} -> {Path(audio_path).name}")
            # ä½¿ç”¨ subprocess.run æ—¶æ•è· stderr ä»¥ä¾¿æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                print(f"[Error] FFmpeg æå–éŸ³é¢‘å¤±è´¥ (é€€å‡ºç  {result.returncode}): {result.stderr}")
                return False
            return True
        except Exception as e:
            print(f"[Error] éŸ³é¢‘æå–å‘ç”Ÿå¼‚å¸¸: {e}")
            return False

    def analyze_audio(self, video_path: str, output_dir: str) -> Optional[List[Dict]]:
        """è°ƒç”¨é˜¿é‡Œäº‘ DashScope ASR æœåŠ¡è¿›è¡Œè¯†åˆ«ã€‚"""
        temp_audio_dir = Path(output_dir) / "temp_audio"
        temp_audio_dir.mkdir(exist_ok=True)
        audio_path = temp_audio_dir / "full_audio.wav"
        
        if not self.extract_audio_track(video_path, str(audio_path)):
            # æ¸…ç†ç›®å½•
            try: shutil.rmtree(temp_audio_dir)
            except: pass
            return None

        print(f"[Analysis] æ­£åœ¨é€šè¿‡ DashScope å¤„ç†éŸ³é¢‘: {audio_path.name}")
        
        if not self.api_key:
            print("[Error] æœªé…ç½® DASHSCOPE_API_KEYï¼Œæ— æ³•è¿›è¡Œ ASR è¯†åˆ«")
            return None

        # 1. è¯­éŸ³è¯†åˆ« (ä½¿ç”¨ DashScope Base64 ç›´æ¥æäº¤)
        try:
            import base64
            print(f"[ASR] æ­£åœ¨è¯»å–éŸ³é¢‘å¹¶è¿›è¡Œ Base64 ç¼–ç ...")
            with open(str(audio_path), "rb") as f:
                audio_base64 = base64.b64encode(f.read()).decode("utf-8")
            
            # æäº¤ ASR ä»»åŠ¡
            asr_response = self._submit_asr_task(audio_base64)
            if not asr_response:
                print("[Error] ASR è¯†åˆ«å¤±è´¥")
                return None
            
            # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯ TaskIDï¼Œéœ€è¦è½®è¯¢
            if isinstance(asr_response, str):
                print(f"[ASR] ä»»åŠ¡å·²æäº¤, TaskID: {asr_response}, æ­£åœ¨ç­‰å¾…ç»“æœ...")
                result_data = self._wait_for_asr_result(asr_response)
            else:
                # å¦åˆ™è¯´æ˜æ˜¯åŒæ­¥è¿”å›çš„ç»“æœ
                print(f"[ASR] æ”¶åˆ°åŒæ­¥è¿”å›ç»“æœ")
                result_data = asr_response

            if not result_data:
                return None
                
            # 2. è§£æç»“æœ
            results = []
            output = result_data.get("output", {})
            # å…¼å®¹ä¸åŒæ¨¡å‹çš„å“åº”ç»“æ„ (æœ‰äº›åœ¨ output.sentences, æœ‰äº›åœ¨ output.results[0].sentences)
            sentences = output.get("sentences")
            if sentences is None:
                res_list = output.get("results", [])
                if res_list:
                    sentences = res_list[0].get("sentences", [])
                else:
                    sentences = []
            
            for s in sentences:
                # è®°å½•å¥å­çº§çš„æ—¶é—´æˆ³
                item = {
                    'start': s.get('begin_time'),
                    'end': s.get('end_time'),
                    'text': s.get('text', '').strip(),
                    'words': [] # è®°å½•è¯çº§æ—¶é—´æˆ³ç”¨äºæ›´ç²¾ç¡®çš„æˆªå›¾
                }
                
                # å°è¯•è·å–è¯çº§æ—¶é—´æˆ³ (timestamp_alignment_enabled å¼€å¯æ—¶è¿”å›)
                words = s.get('words', [])
                if words:
                    for w in words:
                        item['words'].append({
                            'text': w.get('text'),
                            'start': w.get('begin_time'),
                            'end': w.get('end_time')
                        })
                
                if item['text']:
                    s_s = item['start'] / 1000.0
                    s_e = item['end'] / 1000.0
                    print(f"  [{s_s:.2f}s - {s_e:.2f}s]: {item['text']}")
                    results.append(item)
            
            return results

        except Exception as e:
            print(f"[Error] ASR è¯†åˆ«å¤±è´¥: {e}")
            traceback.print_exc()
            return None
        finally:
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            try: shutil.rmtree(temp_audio_dir)
            except: pass

    def _submit_asr_task(self, audio_base64: str) -> Optional[Union[str, Dict]]:
        """æäº¤ ASR ä»»åŠ¡ï¼Œæ”¯æŒå¼‚æ­¥å’ŒåŒæ­¥è¿”å›ã€‚"""
        import sys
        url = "https://dashscope.aliyuncs.com/api/v1/services/audio/asr/transcription"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "X-DashScope-Async": "enable"
        }
        # ä½¿ç”¨ audio å­—æ®µä¼ é€’ Base64 æ•°æ®
        payload = {
            "model": "fun-asr-mtl-2025-08-25",
            "input": {
                "audio": audio_base64,
                "sample_rate": 16000
            },
            "parameters": {
                "language_hints": ["zh", "en"],
                "timestamp_alignment_enabled": True,
                "rich_transcription_enabled": True
            }
        }
        try:
            print(f"[ASR] æ­£åœ¨æäº¤ä»»åŠ¡ (Base64 æ–¹å¼)...")
            sys.stdout.flush()
            # å¢åŠ è¶…æ—¶æ—¶é—´ï¼ŒBase64 ä¼ è¾“è¾ƒæ…¢
            resp = requests.post(url, headers=headers, json=payload, timeout=60)
            if resp.status_code != 200:
                print(f"[ASR Error] æäº¤å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}, è¯¦æƒ…: {resp.text}")
                sys.stdout.flush()
                return None
            
            res = resp.json()
            task_id = res.get("output", {}).get("task_id")
            if task_id:
                return task_id
            
            # å¦‚æœæ²¡æœ‰ task_id ä½†æœ‰ outputï¼Œå¯èƒ½æ˜¯åŒæ­¥è¿”å›
            if "output" in res:
                return res
                
            print(f"[ASR Error] å“åº”æ ¼å¼æœªçŸ¥: {res}")
            sys.stdout.flush()
            return None
        except Exception as e:
            print(f"[ASR Error] æäº¤ä»»åŠ¡å¤±è´¥: {e}")
            sys.stdout.flush()
        return None

    def _wait_for_asr_result(self, task_id: str) -> Optional[Dict]:
        """è½®è¯¢ ASR ä»»åŠ¡ç»“æœã€‚"""
        import sys
        url = f"https://dashscope.aliyuncs.com/api/v1/tasks/{task_id}"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        
        print(f"[ASR] å¼€å§‹è½®è¯¢ä»»åŠ¡ç»“æœ (Task ID: {task_id})...")
        max_retries = 60 # æœ€å¤šç­‰ 60 ç§’
        for i in range(max_retries):
            try:
                resp = requests.get(url, headers=headers, timeout=10)
                resp.raise_for_status()
                res = resp.json()
                status = res.get("output", {}).get("task_status")
                
                if i % 10 == 0: # æ¯ 10 ç§’æ‰“å°ä¸€æ¬¡çŠ¶æ€
                    print(f"[ASR] è½®è¯¢ä¸­... å½“å‰çŠ¶æ€: {status}")
                    sys.stdout.flush()

                if status == "SUCCEEDED":
                    print(f"[ASR Success] ä»»åŠ¡å®Œæˆï¼")
                    sys.stdout.flush()
                    return res
                elif status in ["FAILED", "CANCELED"]:
                    print(f"[ASR Error] ä»»åŠ¡çŠ¶æ€å¼‚å¸¸: {status}, è¯¦æƒ…: {res}")
                    sys.stdout.flush()
                    return None
                
                time.sleep(1)
            except Exception as e:
                print(f"[ASR Error] è½®è¯¢ç»“æœå¤±è´¥: {e}")
                sys.stdout.flush()
                time.sleep(1)
        
        print("[ASR Error] ä»»åŠ¡è¶…æ—¶")
        sys.stdout.flush()
        return None

    def _get_anchors(self, results: List[Dict], video_path: str) -> List[float]:
        """åŸºäºè¯­éŸ³å’Œè§†è§‰å˜åŒ–ç”Ÿæˆé”šç‚¹ã€‚"""
        anchors = []
        if not results:
            return []

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)
            
        if not cap.isOpened():
            print(f"[Error] æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return []
            
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        # 1. åŸºäºè¯­éŸ³çš„é”šç‚¹
        print("[Anchors] æ­£åœ¨ç”ŸæˆåŸºäºè¯­éŸ³çš„é”šç‚¹...")
        for res in results:
            # ä¼˜å…ˆä½¿ç”¨è¯çº§æ—¶é—´æˆ³ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'words' in res and res['words']:
                # åœ¨æ¯ä¸ªå¥å­çš„å¼€å¤´å’Œç»“å°¾æ‰“æ¡©
                first_word = res['words'][0]
                last_word = res['words'][-1]
                
                # å¥å­å¼€å¤´
                anchors.append(round(first_word['start'] / 1000.0 + 0.1, 2))
                # å¥å­ç»“å°¾
                anchors.append(round(last_word['end'] / 1000.0 - 0.1, 2))
                
                # å¦‚æœå¥å­å¾ˆé•¿ï¼Œåœ¨ä¸­é—´ä¹Ÿæ‰“æ¡©
                if len(res['words']) > 10:
                    mid_word = res['words'][len(res['words']) // 2]
                    anchors.append(round(mid_word['start'] / 1000.0, 2))
            else:
                # å›é€€åˆ°å¥å­çº§æ—¶é—´æˆ³
                start_s = res['start'] / 1000.0
                end_s = res['end'] / 1000.0
                
                s_anchor = round(start_s + 0.3, 2)
                e_anchor = round(end_s - 0.2, 2)
                
                if e_anchor > s_anchor:
                    anchors.append(s_anchor)
                    anchors.append(e_anchor)
                else:
                    anchors.append(round((start_s + end_s) / 2, 2))

        # 2. è§†è§‰å˜åŒ–æ£€æµ‹
        print("[Anchors] æ­£åœ¨æ£€æµ‹è§†è§‰å˜åŒ–...")
        sample_rate = 2 
        last_frame_gray = None
        for t in np.arange(0, duration, 1.0 / sample_rate):
            cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)
            ret, frame = cap.read()
            if not ret: break
            
            curr_gray = cv2.cvtColor(cv2.resize(frame, (64, 64)), cv2.COLOR_BGR2GRAY)
            if last_frame_gray is not None:
                diff = cv2.absdiff(curr_gray, last_frame_gray)
                score = np.mean(diff)
                if score > 30:
                    anchors.append(max(0, round(t - 0.1, 2)))
                    anchors.append(min(duration - 0.01, round(t + 0.1, 2)))
            last_frame_gray = curr_gray
        
        cap.release()

        final_anchors = sorted(list(set([a for a in anchors if 0 <= a < duration])))
        print(f"[Anchors] æ€»é”šç‚¹æ•°: {len(final_anchors)}")
        return final_anchors

    def extract_frames(self, video_path: str, anchors: List[float], temp_dir: str) -> List[Tuple[float, str]]:
        """åœ¨é”šç‚¹å¤„æå–å¸§ã€‚"""
        frame_paths = []
        temp_path = Path(temp_dir)
        temp_path.mkdir(exist_ok=True)
        
        for i, ts in enumerate(anchors):
            out_path = temp_path / f"frame_{i:04d}.jpg"
            cmd = [
                str(self.ffmpeg_exe), "-y", "-ss", str(ts), "-i", video_path,
                "-vframes", "1", "-q:v", "2", str(out_path), "-loglevel", "error"
            ]
            try:
                subprocess.run(cmd, check=True)
                if out_path.exists():
                    frame_paths.append((ts, str(out_path)))
            except Exception as e:
                print(f"[FFmpeg Error] å¤±è´¥äº {ts}s: {e}")
        return frame_paths

    def _get_hashes(self, img: np.ndarray) -> Dict:
        """è®¡ç®— aHash, dHash, pHashã€‚"""
        resized = cv2.resize(img, (256, 256))
        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (3, 3), 0)

        # aHash
        ahash_img = cv2.resize(blurred, (8, 8), interpolation=cv2.INTER_AREA)
        avg = ahash_img.mean()
        ahash = "".join(['1' if p > avg else '0' for p in ahash_img.flatten()])

        # dHash
        dhash_img = cv2.resize(blurred, (9, 8), interpolation=cv2.INTER_AREA)
        dhash = ""
        for i in range(8):
            for j in range(8):
                dhash += "1" if dhash_img[i, j] > dhash_img[i, j+1] else "0"

        # pHash
        phash_img = cv2.resize(blurred, (32, 32), interpolation=cv2.INTER_AREA)
        dct = cv2.dct(np.float32(phash_img))
        dct_low = dct[:8, :8]
        p_avg = dct_low.mean()
        phash = "".join(['1' if p > p_avg else '0' for p in dct_low.flatten()])

        return {'ahash': ahash, 'dhash': dhash, 'phash': phash, 'raw_gray': blurred}

    def _get_multi_distance(self, h1: Dict, h2: Dict) -> Dict:
        """è®¡ç®—å“ˆå¸Œä¹‹é—´çš„è·ç¦»ã€‚"""
        d_a = sum(c1 != c2 for c1, c2 in zip(h1['ahash'], h2['ahash']))
        d_d = sum(c1 != c2 for c1, c2 in zip(h1['dhash'], h2['dhash']))
        d_p = sum(c1 != c2 for c1, c2 in zip(h1['phash'], h2['phash']))
        
        pixel_diff = np.mean(cv2.absdiff(h1['raw_gray'], h2['raw_gray']))
        
        return {
            'ahash': d_a, 'dhash': d_d, 'phash': d_p, 
            'avg': (d_a + d_d + d_p) / 3.0,
            'pixel_diff': pixel_diff
        }

    def _cv2_imread_unicode(self, path: str) -> Optional[np.ndarray]:
        """æ”¯æŒ unicode è·¯å¾„è¯»å–å›¾åƒã€‚"""
        try:
            return cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_COLOR)
        except Exception as e:
            print(f"[Error] è¯»å–å›¾åƒå¤±è´¥ {path}: {e}")
            return None

    def remove_duplicate_frames(self, frame_info: List[Tuple[float, str]], threshold: int = 5) -> List[Tuple[float, str]]:
        """ç§»é™¤é‡å¤å¸§ã€‚"""
        if not frame_info: return []
        
        print(f"[Dedup] æ­£åœ¨å»é‡ {len(frame_info)} å¸§ (é˜ˆå€¼: {threshold})...")
        
        frame_hashes = []
        for ts, path in frame_info:
            img = self._cv2_imread_unicode(path)
            if img is not None:
                frame_hashes.append({'ts': ts, 'path': path, 'hashes': self._get_hashes(img)})
            else:
                print(f"[Warning] æ— æ³•è¯»å–å›¾åƒç”¨äºå»é‡: {path}")

        if not frame_hashes: return []

        kept = [frame_hashes[0]]
        report_data = []
        filtered_pairs_count = 0

        for i in range(1, len(frame_hashes)):
            curr = frame_hashes[i]
            prev = kept[-1]
            
            dist = self._get_multi_distance(curr['hashes'], prev['hashes'])
            
            is_different = (
                dist['ahash'] > threshold or 
                dist['dhash'] > threshold or 
                dist['phash'] > threshold or 
                dist['avg'] > 4 or
                dist['pixel_diff'] > 15
            )
            
            if is_different:
                kept.append(curr)
            else:
                filtered_pairs_count += 1
            
            report_data.append({
                'ts_pair': (prev['ts'], curr['ts']),
                'distances': dist,
                'kept': is_different
            })

        final_list = kept
        if len(final_list) > 9:
            indices = np.linspace(0, len(final_list) - 1, 9).astype(int)
            final_list = [final_list[idx] for idx in indices]
            print(f"[Dedup] å¸§æ•°è¿‡å¤š ({len(kept)} > 9)ï¼Œå·²é‡é‡‡æ ·è‡³ 9ã€‚")
        
        elif len(final_list) < 3 and len(frame_hashes) >= 3:
            existing_paths = {f['path'] for f in final_list}
            candidates = [f for f in frame_hashes if f['path'] not in existing_paths]
            while len(final_list) < 3 and candidates:
                final_list.append(candidates.pop(len(candidates)//2))
            final_list.sort(key=lambda x: x['ts'])
            print(f"[Dedup] å¸§æ•°è¿‡å°‘ ({len(kept)} < 3)ï¼Œå·²ä»åŸå§‹å¸§è¡¥å……ã€‚")

        print(f"[Dedup] {len(frame_info)} -> {len(final_list)} (è¿‡æ»¤äº† {filtered_pairs_count})")
        
        # æ¸…ç†åˆ é™¤çš„æ–‡ä»¶
        final_paths = {f['path'] for f in final_list}
        for ts, p in frame_info:
            if p not in final_paths:
                try: Path(p).unlink()
                except: pass

        # ç”ŸæˆæŠ¥å‘Š
        report_path = Path(frame_info[0][1]).parent.parent / "dedup_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=== å»é‡æŠ¥å‘Š ===\n")
            f.write(f"åŸå§‹: {len(frame_info)}\n")
            f.write(f"ä¿ç•™: {len(final_list)}\n\n")
            for item in report_data:
                f.write(f"[{item['ts_pair'][0]:.2f}s vs {item['ts_pair'][1]:.2f}s] ")
                f.write(f"pHash:{item['distances']['phash']} dHash:{item['distances']['dhash']} ")
                f.write(f"Diff:{item['distances']['pixel_diff']:.2f} Keep:{item['kept']}\n")
        
        return [(f['ts'], f['path']) for f in final_list]

    def create_contact_sheet(self, frame_info: List[Tuple[float, str]], output_base_path: str) -> List[str]:
        """åˆ›å»ºæ‹¼å›¾ (ä¹å®«æ ¼)ã€‚"""
        if not frame_info: return []
            
        total_frames = len(frame_info)
        chunk_size = 9
        output_files = []

        with Image.open(frame_info[0][1]) as first_img:
            w, h = first_img.size
            is_portrait = h > w

        for chunk_idx, i in enumerate(range(0, total_frames, chunk_size)):
            chunk = frame_info[i : i + chunk_size]
            num_in_chunk = len(chunk)
            
            if num_in_chunk <= 3:
                cols, rows = (1, num_in_chunk) if not is_portrait else (num_in_chunk, 1)
            elif num_in_chunk <= 4:
                cols, rows = 2, 2
            elif num_in_chunk <= 6:
                cols, rows = (2, 3) if not is_portrait else (3, 2)
            else:
                cols, rows = 3, 3

            max_side = 400
            processed_imgs = []
            try:
                font = ImageFont.truetype("arial.ttf", 22)
            except:
                font = ImageFont.load_default()

            for ts, p in chunk:
                with Image.open(p) as img:
                    img = img.convert("RGB")
                    img.thumbnail((max_side, max_side))
                    
                    draw = ImageDraw.Draw(img, "RGBA")
                    ts_text = f"{ts:.2f}s"
                    
                    bbox = draw.textbbox((5, 5), ts_text, font=font)
                    draw.rectangle([bbox[0]-2, bbox[1]-2, bbox[2]+2, bbox[3]+2], fill=(0,0,0,160))
                    draw.text((5, 5), ts_text, fill="white", font=font)
                    processed_imgs.append(img.copy())

            cell_w, cell_h = processed_imgs[0].size
            canvas = Image.new('RGB', (cell_w * cols, cell_h * rows), (30, 30, 30))
            for idx, img in enumerate(processed_imgs):
                canvas.paste(img, ((idx % cols) * cell_w, (idx // cols) * cell_h))

            if total_frames <= chunk_size:
                save_path = output_base_path
            else:
                sheet_name = f"contact_sheet_{chunk_idx + 1}.jpg"
                save_path = output_base_path.replace("final_sheet.jpg", sheet_name)
            
            canvas.save(save_path, "JPEG", quality=95)
            print(f"[Success] æ‹¼å›¾å·²ä¿å­˜: {save_path}")
            output_files.append(save_path)

        return output_files

def process_video_folder(video_folder: Path, output_root: Path, progress_callback=None):
    """å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘ã€‚"""
    analyzer = VideoAnalyzer()

    valid_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.ts')
    
    if not video_folder.exists():
        print(f"[Error] è§†é¢‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {video_folder}")
        if progress_callback:
            progress_callback(f"âŒ è§†é¢‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {video_folder}")
        return

    video_files = [f for f in video_folder.iterdir() if f.suffix.lower() in valid_extensions]
    
    if not video_files:
        print(f"[Warning] æœªæ‰¾åˆ°æœ‰æ•ˆè§†é¢‘: {video_folder}")
        if progress_callback:
            progress_callback(f"âš ï¸ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆè§†é¢‘: {video_folder}")
        return

    print(f"[Batch] å‘ç° {len(video_files)} ä¸ªè§†é¢‘")
    
    # é˜¶æ®µ 1: éŸ³é¢‘æå– & ASR
    if progress_callback:
        progress_callback(f"ğŸµ æ­£åœ¨æå–éŸ³é¢‘å¹¶è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œå…±è®¡ {len(video_files)} æ¡...")

    audio_success_count = 0
    for video_file in video_files:
        video_name = video_file.name
        video_basename = video_file.stem
        
        video_out_dir = output_root / video_basename
        video_out_dir.mkdir(parents=True, exist_ok=True)
        
        transcript_path = video_out_dir / "transcript_detailed.txt"
        
        # æ£€æŸ¥å­—å¹•æ˜¯å¦å­˜åœ¨
        if transcript_path.exists():
            audio_success_count += 1
            continue

        print(f"\n>>> æ­£åœ¨å¤„ç†éŸ³é¢‘: {video_name}")
        results = analyzer.analyze_audio(str(video_file), str(video_out_dir))
        
        if results:
            with open(transcript_path, "w", encoding="utf-8") as f:
                for item in results:
                    f.write(f"[{item['start']/1000:.2f}s - {item['end']/1000:.2f}s] {item['text']}\n")
            audio_success_count += 1
        else:
            print(f"[Skip] æœªæ£€æµ‹åˆ°è¯­éŸ³æˆ–éŸ³é¢‘å¤±è´¥: {video_name}")
            if progress_callback:
                progress_callback(f"âš ï¸ éŸ³é¢‘æå–å¤±è´¥: {video_name}")

        # é˜¶æ®µ 2: æˆªå›¾
        if progress_callback:
            progress_callback(f"ğŸ–¼ï¸ æ­£åœ¨è¿›è¡Œè§†é¢‘æˆªå›¾...")
            
        video_out_dir = output_root / video_basename
        image_out_dir = video_out_dir / "cache_images"
        sheet_path = video_out_dir / "final_sheet.jpg"
        
        if not sheet_path.exists() and results:
            print(f"\n>>> æ­£åœ¨å¤„ç†å›¾åƒ: {video_name}")
            anchors = analyzer._get_anchors(results, str(video_file))
            frame_info = analyzer.extract_frames(str(video_file), anchors, str(image_out_dir))
            
            if frame_info:
                final_frames = analyzer.remove_duplicate_frames(frame_info)
                analyzer.create_contact_sheet(final_frames, str(sheet_path))
                print(f"[Done] å®Œæˆå›¾åƒå¤„ç†: {video_name}")
            else:
                print(f"[Warning] æœªæå–åˆ°æœ‰æ•ˆå¸§: {video_name}")

        # --- è‡ªåŠ¨åˆ é™¤è§†é¢‘ä»¥èŠ‚çœç©ºé—´ ---
        try:
            print(f"[Cleanup] æ­£åœ¨åˆ é™¤ä¸´æ—¶è§†é¢‘: {video_name}")
            video_file.unlink()
        except Exception as e:
            print(f"[Cleanup Error] åˆ é™¤å¤±è´¥ {video_name}: {e}")

    analyzer.release_model()
    if progress_callback:
        progress_callback("âœ… è§†é¢‘é¢„å¤„ç†ï¼ˆéŸ³é¢‘+æˆªå›¾ï¼‰å…¨éƒ¨å®Œæˆï¼")

